Task 3 assigned on Sunday, April 20, 2025
Steps I followed
1. Study the dataset and it's column descriptions
2. Downloaded the dataset
3. Maintained a folder to keep the data and jupyter notebook for task 3 and EDA to be done 
4. Setup a virtual environment for the EDA task 
5. Activated the virtual environment and installed necessary packages for EDA like: pandas, matplotlib, numpy, seaborn, plotly, streamlit
6. Load the dataset in a pandas dataframe : df 
7. Explored the dataframes few rows using : df.head()
8. Checked the dataframe for null/missing values using : isnull() -> output: 5 columns contained missing values
9. Explored those 5 columns for distinct values they contain : acqCountry, mechantCountryCode, posEntryMode, posConditionCode, transactionType
10. All 5 columns wer categorial columns for used mode for imputation and handled the missing values: fillna(), mode()
11. Check for duplicates: df.duplicated().sum() -> no duplicates found
12. For checking outliers in numerical columns: creditLimit, availableMoney, transactionAmount, currentBalance, first i explored if there columns are normally distributed or not 
13. I ploted histogram: histplot() -> there 4 numerical columns are positively skewed (peak on the left, tail stretching to the right)
14. So for outlier detection used : IQR -> (Q1 - 1.5 * IQR) , (Q3 + 1.5 * IQR), values not falling in this ranger are considered outliers
15. Visualized outliers -> Boxplot of creaditLimit, availableMoney, transactionAmount, currentBalance
16. Also viewed outliers from different approach -> Transaction Amount > Available Money and plotted scatterplot
17. Summary Statistics and Distribution for those 4 columns -> describe() -> (count, mean, std, min, max, 25%, 50%, 75%)
18. Correlation Analysis -> Correlation Matrix of (creaditLimit, availableMoney, transactionAmount, currentBalance) -> it is found that creditLimit and availableMoney are higly correlated, also creditLimit and currentBalance are also moderately correlated -> sns.heatmap( correlation_matrix..)
19. Plotted a scatterplot availableMoney vs creditLimit that also showed the correlation -> increasing creaditlimit increase the availableMoney 
20. For time-based analysis first checked the datatype of 24 columns and converted dates to datetimeformat -> to_datetime()
        a. Trend of Transactions Over time -> line graph -> showing monthly and weekly transaction trend -> max transactions in Octover month, max transaction in fourth week of december
        b. Hour of Day when most transaction and frauds happen -> bar chart, most transactions hour : 16 hour, most fraud transaction hour : 23 hour
        c. Most active day of week-> bar plot -> max transaction: thursday, max frauds: saturday
21. Geographic analysis :
        a. Transaction frequency and transaction amount by country: US on the top, Mexio second and then canada and at last Puerto Rico
        b. Fraud rate by country -> bar plt ->  max in Canada, then in Puerto
22. Merchant Analysis:
        a. Top merchantName and merchantCategory by transaction frequency and amount :> top merchantName: Lyft, uber... , top merchantCategory: online_retail, fastfood, food,..
        b. Fraud rate by merchantCategoryCode -> top ones are : hotels, online_retail, rideshare...
23. Fraud Pattern Identification
        a. posEntryMode and posConditionCode combinations that can be risky: 90, 99 (QR or others, unknown)
        b. Fraud rate by transactionType -> top : REVERSAL, then PURCHASE, THEN ADDRESS_VERIFICATION
        c. Fraud rate by cardPresent flag -> cardNotPresent have higher (1.95) while cardPresent have (1.55)%
        d. Repeated frauds from same customerId or accountNumber (here both are same): 
24.     i. First listed all accounts whose isFraud is true
        ii. Createad a copy of dataframe containing only those accounts rows that are risky
        iii. then explored for most frequent merchantName in that dataframe -> Uber, Lyft, alibaba.com..so on
        iv. most frequent merchants category in repeated frauds ->online_retail, fastfood ...
        v. most common transaction amount -> many times 0.0 amount, then 171.51, 212.60 similary..
        vi. transaction type used by repeated fraud accounts -> purchase (highest)
        vii. POS Entry modes involved in repeated frauds: -> 9 ( manual card details entry), 2 (Magnetic stripe swipe — Older method), 5 (chip read)
        viii. Merchant countries invovled in repeated frauds -> US 
        ix. Average time gap between account opening and transaction date -: 30.45 months

25. Checking if mismathes in CVV or Expiration date correlate with fraud -> CVV mismatch fraud rate (3.28%), Expiration date mismatch fraud rate (1.76%)
26. Spending behavior : Fraud vs Non-fraud : clustered column chart ->In fraud as the availableMoney grows the transaction amount also grows, while in non fraud its remains normal,
again plotted in a bar and line graph combined then it showed that: Users with mid-level available money (20K–30K) seem to be more vulnerable to fraud, even though their transaction behavior is similar to others, also 
Interestingly, the fraud rate drops in the highest available money bin (40K–50K).



What challenges I encountered: 
1. Not sure which visualization charts to use in many cases, so I performed just hit and terminal
2. Familiar to just few functions of pandas, matplotlib, and seaborn, so used ChatGPT and felt a need to learn about more methods and it's arguments
3. Also posEntryMode, posConditionCode were completely new to me , from google learnt basis code and their meanings
4. I handled the missing data by replacing with mode values, but not sure if i was efficient method or not
5. While drawing insights and pattern, i randomly choosed columns and get started, not sure if I need to follow or give priority to certain columns first
6. Boxplot was new to me , took time to understand this
7. Also for outlier detection, I first checked for the distribution of numerical columns


what I need to improve -> inspired from Intern Mate -> Bishnu
1. which account number does most transactions
2. availableMoney negative value why?
3. outlier range outlier ma kun 
4. std high then mean why, what happens : what does it signifies
5. mean get affected by outliers, mean says what?
6. each ko Statistics herne ke bhanxh batau necessary
7. time analysis -> no of transaction in each month, and fraud transaction in each month, which is highest
8. no of trasnaction is highest at 4 pm evening and whereas lowest at 2 pm.. similary
9. at which time fraud trasnaction occur, and which normal time
10. which day transaction amount high and when lowest, 
11. which day transaction count high and low,
12. which day fraud trasnaction 
13. pareto, log normality what are these
14. normal average transaction amount vs fraud average transaction amount
15. merchant name contained # -> merchantBrand ma changed
16. merchant how many free from fraud, number, chi square used
17, which have highest trasnactoin and which are fruad prone
18. pie char used for fradulent transaction for country
19. kati ota transaction kun posEntryMode baat aayo, most common kun ho 
20. most common trasnaction in posConditionCode 


futher suggestion:
1. mulltivariate
2. customer ani transaction data both in the dataset so explored
3. isFraud for customer or transaction bujhne
4. customer le kati trasnaction kun ma garxh
5. customer perspective ma ni herne along with transaction perspective
6. 

