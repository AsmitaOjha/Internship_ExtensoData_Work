ðŸ“˜ Confusion Matrix & Evaluation Metrics â€” Summary for Binary Classification

# ðŸ”² Confusion Matrix Format

|                 | Predicted: No (0)   | Predicted: Yes (1)  |
| --------------- | --------------------| ------------------- |
| Actual: No (0)  | TN (True Negative)  | FP (False Positive) |
| Actual: Yes (1) | FN (False Negative) | TP (True Positive)  |

---

# ðŸ“Œ Metrics Formulas

1. Precision

> Measures how many predicted positives were correct.

Precision = TP / (TP + FP)

2. Recall (Sensitivity or True Positive Rate)

> Measures how many actual positives were correctly predicted.

Recall = TP / (TP + FN)

3. F1-Score

> Harmonic mean of Precision and Recall (balances both).

F1 Score = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
4. Accuracy

> Overall correctness of the model.

Accuracy = (TP + TN) / (TP + TN + FP + FN)

5. Support
> Total actual occurrences of each class.
Support (class 0) = TN + FP
Support (class 1) = TP + FN


---

# âœ… Example Values (for reference)

If confusion matrix is:


[[83965 42158]
 [  854  1406]]


Then:

* Precision â‰ˆ 1406 / (1406 + 42158) â‰ˆ 0.032
* Recall â‰ˆ 1406 / (1406 + 854) â‰ˆ 0.622
* F1 Score â‰ˆ 2 Ã— (0.032 Ã— 0.622) / (0.032 + 0.622) â‰ˆ 0.06
* Accuracy â‰ˆ (83965 + 1406) / 128383 â‰ˆ 0.665
* Support for class 0 = 83965 + 42158 = 126123
* Support for class 1 = 854 + 1406 = 2260



